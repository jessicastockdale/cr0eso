---
title: "Additional analysis of BC LTHC outbreak data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Additional analysis of BC LTHC outbreak data}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(dev="png", collapse = T, comment = "#>")
options(tibble.print_min = 4L, tibble.print_max = 4L)

# load libraries
library(tidyverse)
library(kableExtra)
library(here)
library(R0)
library(ggsidekick)
library(ggstance)
library(scales)
library(lubridate)
library(gridExtra)

library("cr0eso")

# Set folder to store outputs
outdir <- here()

set.seed(836361)

# To get the colour scheme used in the paper, uncomment the below:
#devtools::install_github('Mikata-Project/ggthemr')
#library(ggthemr)
#ggthemr("fresh")
#palette(swatch())
# As of the time of writing, there is a bug in this library introduced by an update to ggplot. To use this colour scheme currently,  add "+ scale_colour_ggthemr_d()" to all ggplot calls. See https://github.com/Mikata-Project/ggthemr/issues/44. 
```

## BC Long Term Health Care outbreak data

The data is formatted as a list with 100 imputations of the missing symptom onset times. For each imputation, the list contains data on the number of cases, time series of cases (by symptom onset), the facility capacity and the outbreak reported date for each facility outbreak. It also contains the time series of cases as a matrix, but we will not use that here. 

```{r}
# View the first imputation of missing data:
BC_LTHC_outbreaks_100Imputs[[1]]

```
## Estimate R0 in each outbreak, using EG and ML methods

For each of the 100 imputations, we estimate R0 independently in each facility using exponential growth (EG) and maximum likelihood (ML) methods, with the 'R0' library.

```{r include = FALSE}

# This chunk may take several minutes to run. It will also output some warning messages - that's ok.  

nIts <- length(BC_LTHC_outbreaks_100Imputs)
res <- vector(mode = "list", length = nIts)

for (its in 1:nIts){

# Generation time distribution
mGT<-generation.time("gamma", c(5.2, 1.73))
# as in Ganyani et al, 2020, Singapore mean/sd


# Do EG, ML R0 estimation, using R0 package

# Structure for results; a list with length equal to the number of facilities
# Each of these will go inside 'res'
estr0<-rep(list(1),length(BC_LTHC_outbreaks_100Imputs[[1]]$Location) )

for (i in 1:length(BC_LTHC_outbreaks_100Imputs[[1]]$Location) ){
  
      # Where to end the 'initial exponential period' of the outbreak that we perform estimation over? Set to the latest maximum incidence day. 
      end <- as.numeric(length(BC_LTHC_outbreaks_100Imputs[[its]]$time_series[[i]]) + 1 - 
        which.max(rev(BC_LTHC_outbreaks_100Imputs[[its]]$time_series[[i]])))
      # If the first day is the single maximum incidence day, as occurs in a few imputations of a few of the smaller outbreaks in this dataset, we set 'end' to the end of the outbreak. There is no perfect choice here, judgement recommended. 
      if (end==1){end=as.numeric(length(BC_LTHC_outbreaks_100Imputs[[its]]$time_series[[i]]))}
      
      # ML method doesn't always obtain an estimate - set up the code to continue on if this gives an error
      t <- try(estimate.R(epid = BC_LTHC_outbreaks_100Imputs[[its]]$time_series[[i]],
                          GT = mGT, time.step = 1, 
                          pop.size = BC_LTHC_outbreaks_100Imputs[[its]]$capacity[i],
                          begin = 1, end = end,
                          methods=c("EG", "ML")))
      
      # If it gives an error, run EG method only
      if("try-error" %in% class(t)){estr0[[i]] <- estimate.R(epid = 
                              BC_LTHC_outbreaks_100Imputs[[its]]$time_series[[i]], 
                              GT = mGT, time.step = 1, pop.size =
                              BC_LTHC_outbreaks_100Imputs[[its]]$capacity[i],
                              begin = 1, end = end,methods=c("EG"))}else{
     # Else run both EG and ML
                              estr0[[i]] <- estimate.R(epid = 
                              BC_LTHC_outbreaks_100Imputs[[its]]$time_series[[i]], 
                              GT = mGT, time.step = 1, begin = 1, end = end, pop.size =
                              BC_LTHC_outbreaks_100Imputs[[its]]$capacity[i],
                              methods=c("EG", "ML"))}
      
}

names(estr0) <- paste("Facility", BC_LTHC_outbreaks_100Imputs[[its]]$Location, sep="_")

res[[its]]<-estr0

}

```

## Output files and summary 

We collect summaries of the R0 estimates from a single outbreak (the final imputation, we use this as the 'main' result), and calculate confidence intervals across all 100 imputations for a sensitivity analysis. 

```{r gather output, warning = FALSE}

# Collect the results into a data frame and save them to file 

# First, save the complete nIts sets of estimates to file (the sensitivity analysis) 
# We want the mean of the 100 means, and the CI on those 100 means, per outbreak
er_df <- data.frame(Location=names(estr0),
                    EG_mean=c(rep(NA, length(names(estr0)))),
                    EG_mean_CI_lower=c(rep(NA, length(names(estr0)))),
                    EG_mean_CI_upper=c(rep(NA, length(names(estr0)))),
                    ML_mean=c(rep(NA, length(names(estr0)))),
                    ML_mean_CI_lower=c(rep(NA, length(names(estr0)))),
                    ML_mean_CI_upper=c(rep(NA, length(names(estr0)))) )


# To calculate the confidence intervals on the means
normConfInt <- function(x, alpha = 0.05){
  mean(x) + qt(1 - alpha / 2, length(x) - 1) * sd(x) * c(-1, 1)}

for (i in 1:length(estr0)){
  a<-NULL; b<-NULL;
  for (j in 1:nIts){
  a <- c(a, res[[j]][[i]]$estimates$EG[1])
  b <- c(b, res[[j]][[i]]$estimates$ML[1])
  }
  er_df[i, 2] <- mean(unlist(a))
  er_df[i, 3] <- normConfInt(unlist(a))[1]
  er_df[i, 4] <- normConfInt(unlist(a))[2]
  er_df[i, 5] <- mean(unlist(b))
  er_df[i, 6] <- normConfInt(unlist(b))[1]
  er_df[i, 7] <- normConfInt(unlist(b))[2]
}
write.table(er_df, file = paste0(outdir,"/R0estimation_results_multiple.txt"))

# show table of results
er_df %>%
  kableExtra::kable() %>%
  kableExtra::kable_styling(bootstrap_options = c("striped","responsive"))



# Second, a single set as the 'main' results - using the final imputation
er_df <- data.frame(Location=names(estr0),
                 EG=c(rep(NA, length(names(estr0)))),
                 EG_CI_lower=c(rep(NA, length(names(estr0)))),
                 EG_CI_upper=c(rep(NA, length(names(estr0)))),
                 ML=c(rep(NA, length(names(estr0)))),
                 ML_CI_lower=c(rep(NA, length(names(estr0)))),
                 ML_CI_upper=c(rep(NA, length(names(estr0)))) )

for (i in 1:length(estr0)){
  er_df[i, 2] <- estr0[[i]]$estimates$EG[1]
  er_df[i, 3] <- estr0[[i]]$estimates$EG$conf.int[1]
  er_df[i, 4] <- estr0[[i]]$estimates$EG$conf.int[2]
  er_df[i, 5] <- ifelse(is.null(estr0[[i]]$estimates$ML[1]),NA,estr0[[i]]$estimates$ML[1])
  er_df[i, 6] <- ifelse(is.null(estr0[[i]]$estimates$ML[1]),NA,estr0[[i]]$estimates$ML$conf.int[1])
  er_df[i, 7] <- ifelse(is.null(estr0[[i]]$estimates$ML[1]),NA,estr0[[i]]$estimates$ML$conf.int[2])
}
write.table(er_df, file=paste0(outdir,"/R0estimation_results_single.txt") )


# show table of results
er_df %>%
  kableExtra::kable() %>%
  kableExtra::kable_styling(bootstrap_options = c("striped","responsive"))




```

## Quick results visualisation

Before we move onto code to recreate the figures in the manuscript, we take a look at some simple plots of the results to confirm they are sensible. 

```{r}

# collect R0 ests with each method (the final imputation iteration)
EG<-0
for (i in 1:length(estr0)){
  EG[i]<-as.numeric(estr0[[i]]$estimates$EG[1])
}
ML<-0
for (i in 1:length(estr0)){
  ML[i]<-min(as.numeric(estr0[[i]]$estimates$ML[1]), 10000)
}
# 10000s are just where ML didn't obtain a solution

# Histograms of th estimates from each method
hist(EG, breaks=20, main="EG R0 estimates", xlab="R0", col="steelblue4")
hist(ML[ML<10000], breaks=20, main="ML R0 estimates", xlab="R0", col="steelblue4")


```

## Attack rates

We calculate the attack rate in each outbreak as the number of cases divided by the facility capacity. We incorporate uncertainty on the attack rate by varying the denominator from 85% to 115% of the known capacity. R0 is also estimated from the attack rate, according to the relation -log(1-A_r)/A_r = R0. 

```{r}
### Attack rates - varying capacity from 85%-115%, and corresponding R0 estimates
AR_table <- cbind("Facility" = as.numeric(paste(BC_LTHC_outbreaks_100Imputs[[100]]$Location)),  
                  "No. cases" = BC_LTHC_outbreaks_100Imputs[[100]]$num_cases,
                  "Capacity" = BC_LTHC_outbreaks_100Imputs[[100]]$capacity,
                  "A_r" = 
                    100*BC_LTHC_outbreaks_100Imputs[[100]]$num_cases/BC_LTHC_outbreaks_100Imputs[[100]]$capacity,
                  "A_r (85% cap.)" =
                    100*BC_LTHC_outbreaks_100Imputs[[100]]$num_cases/(BC_LTHC_outbreaks_100Imputs[[100]]$capacity*0.85),
                  "A_r (115% cap.)" =
                    100*BC_LTHC_outbreaks_100Imputs[[100]]$num_cases/(BC_LTHC_outbreaks_100Imputs[[100]]$capacity*1.15),
                  "R0 (A_r)" =
                    -log(1-(BC_LTHC_outbreaks_100Imputs[[100]]$num_cases/BC_LTHC_outbreaks_100Imputs[[100]]$capacity))/(BC_LTHC_outbreaks_100Imputs[[100]]$num_cases/BC_LTHC_outbreaks_100Imputs[[100]]$capacity),
                  "R0 (A_r, 85% cap.)" =
                    -log(1-(BC_LTHC_outbreaks_100Imputs[[100]]$num_cases/BC_LTHC_outbreaks_100Imputs[[100]]$capacity*0.85))/(BC_LTHC_outbreaks_100Imputs[[100]]$num_cases/BC_LTHC_outbreaks_100Imputs[[100]]$capacity*0.85),
                  "R0 (A_r, 115% cap.)" =
                    -log(1-(BC_LTHC_outbreaks_100Imputs[[100]]$num_cases/BC_LTHC_outbreaks_100Imputs[[100]]$capacity*1.15))/(BC_LTHC_outbreaks_100Imputs[[100]]$num_cases/BC_LTHC_outbreaks_100Imputs[[100]]$capacity*1.15) )


# show table of results
AR_table %>%
  kableExtra::kable() %>%
  kableExtra::kable_styling(bootstrap_options = c("striped","responsive"))


```

## Compare attack rate to R0 estimates from different methods

Compare EG and ML estimates with results from the Bayesian hierarchical model, against the attack rate.  

```{r, fig.width = 7}

AR <- AR_table[,4] # attack rate estimates (base 100% capacity)
ML[ML==10000] <- NA # Return invalid ML estimates to NA

# The Bayesian hierarchical model results (from the manuscript) are manually set here for ease of loading, so remember to update these if you are adapting this script for another analysis. 
BHM <- c(5.8, 2.37, 2.75, 3.43, 6.26, 0.48, 0.99, 5.19, 0.82, 3.07, 10.08, 1.37, 3.61, 0.91, 1.67, 1.6, 6.49, 1.28)
# The following aren't used until the next chunk, but they are also manually entered BHM results:
BHM_multi <- c(9.1, 2.02, 2.78, 2.95, 5.66, 0.53, 0.89, 4.4, 0.75, 3.09, 9.04, 1.72, 3.12, 0.82, 1.46, 1.39, 5.78, 1.45) # multi-level zeta point estimates
BHM_upCI <- c(8.41, 3.47, 3.81, 4.65, 8.16, 1.09, 2.14, 7.62, 1.88, 4.09, 13.02, 2.58, 4.91, 1.65, 2.51, 2.75, 8.49, 2.45) # BHM upper credible interval
BHM_multi_upCI <- c(12.03, 3.09, 3.94, 4.2, 7.95, 1.13, 1.88, 6.79, 1.75, 4.41, 11.86, 2.82, 4.46, 1.48, 2.25, 2.37, 7.86, 2.39) # multi-level zeta upper credible interval
BHM_lowCI <- c(4.27, 1.46, 2.0, 2.51, 4.78, 0.11, 0.3, 3.55, 0.22, 2.24, 7.83, 0.48, 2.66, 0.38, 0.97, 0.7, 5.0, 0.47) # BHM lower credible interval
BHM_multi_lowCI <- c(5.79, 1.23, 2.09, 2.17, 4.32, 0.16, 0.28, 2.99, 0.22, 2.34, 7.19, 0.87, 2.31, 0.33, 0.85, 0.63, 4.47, 0.74) # multi-level zeta lower credible interval


scatterdata <- data.frame(c(AR,AR,AR), c(EG,ML,BHM), c(rep("Exponential growth",length(AR)), rep("Maximum likelihood",length(AR)), rep("Bayesian hierarchical",length(AR))) )
names(scatterdata) <- c("ar", "r0", "Method")

p <- ggplot(scatterdata, aes(x=ar, y=r0, color=Method)) +
  geom_point(size=3, alpha=0.8) +
  geom_smooth(method=lm, se=FALSE, fullrange=TRUE)+
  theme_sleek() + xlab("Attack Rate (%)") + ylab(expression(R['0,k'])) +  theme(legend.position="right", legend.title = element_text(size=10,
                                                                                                               face="bold"), legend.text=element_text(size=10)) +
  annotate(geom="label", x=75, y=9.7, label=round(cor(AR, BHM),2), colour=palette()[2]) +
  annotate(geom="label", x=75, y=1.7, label=round(cor(AR, EG),2), colour=palette()[3])+
  annotate(geom="label", x=75, y=3.5, label=round(cor(AR[!is.na(ML)], ML[!is.na(ML)]),2), colour=palette()[4]) 
p

# The geom_smooth/geom_point warning messages can be safely ignored - these are caused by the missing ML values

```

## Compare R0 estimates from three main methods (BHM, EG, ML)

This figure compares point estimates and confidence intervals/credible intervals from the BHM (single- and multi- level zeta), EG and ML methods. 

``` {r, fig.width = 7, fig.height = 5}

## First combine the results into a data frame
df <- data.frame(c(rep(BC_LTHC_outbreaks_100Imputs[[100]]$Location, 4)),
                 (c(EG, ML ,BHM, BHM_multi)),
                 (c(er_df$EG_CI_upper, er_df$ML_CI_upper, BHM_upCI, BHM_multi_upCI)),
                 (c(er_df$EG_CI_lower, er_df$ML_CI_lower, BHM_lowCI, BHM_multi_lowCI)),
                 c(rep("Exponential growth",
                       length(BC_LTHC_outbreaks_100Imputs[[100]]$Location)),
                   rep("Maximum likelihood",
                       length(BC_LTHC_outbreaks_100Imputs[[100]]$Location)),
                   rep("Bayesian hierarchical",
                       length(BC_LTHC_outbreaks_100Imputs[[100]]$Location)),
                   rep("Bayesian hierarchical, multi-level",
                       length(BC_LTHC_outbreaks_100Imputs[[100]]$Location))))
names(df) <- c("Location", "logR0", "lowerCI", "upperCI", "Method")

# Create the plot:

# We order the locations by increasing BHM R0 estimate
df<- df %>%
  mutate(Location = factor(Location, levels=BC_LTHC_outbreaks_100Imputs[[100]]$Location[order(BHM)] )) %>%
  mutate(Method = factor(Method, levels=c("Bayesian hierarchical", "Exponential growth", "Maximum likelihood", "Bayesian hierarchical, multi-level")))

p<-ggplot(df, aes(y=Location, x=logR0, group=Method, color=Method)) +
  xlab(expression(R['0,k'])) + ylab("Location") +
  geom_errorbarh(mapping=aes(xmin=lowerCI, xmax=upperCI), size = 0.5, alpha=0.99, position=position_dodgev(height=0.8), height=0) +
  geom_point(position=position_dodgev(height=0.8)) + #theme_sleek() +
  theme(legend.position="bottom") +
  guides(colour = guide_legend(nrow = 2)) + scale_x_continuous(trans="log10", limits=c(0.000001,1000), labels=c("0.001","0.01","0.1", "1.0","10","100", "1000"), breaks=c(0.001,0.01, 0.1, 1, 10, 100, 1000))
p




```


## Estimates of the critical time per facility

Using the estimates of zeta from the Bayesian hierarchichal model, we are able to estimate the 'critical time', that is, the length of time from the introduction of outbreak interventions to R0 dropping below 1, in each facility.

(This chunk simply generates the figure, see other vignette for how these critical time estimates are calculated.)

```{r, fig.width = 7}

crit_time <- c(3.91, 1.71, 2.21, 2.73, 4.09, 0.0, 0.0, 3.86, 0.0, 2.28, 4.96, 0.48, 2.73, 0.0, 0.93, 0.81, 3.92, 0.38)
ct_lowCI <- c(0.76, 0.3, 0.39, 0.54, 0.74, 0.11, 0.0, 0.67, 0.0, 0.43, 1.03, 0.0, 0.51, 0.0, 0.0, 0.0, 0.84, 0.0)
ct_highCI <- c(20.22, 9.85, 11.67, 14.27, 21.8, 0.11, 3.83, 19.83, 2.48, 13.78, 26.31, 5.93, 15.43, 2.22, 6.73, 7.32, 22.12, 5.76)
df2 <- data.frame(BC_LTHC_outbreaks_100Imputs[[100]]$Location, crit_time, ct_lowCI, ct_highCI, AR)
names(df2) <- c("Location", "Critical_time", "Lower_CI", "Upper_CI", "Attack_rate")

df2<- df2 %>%
  mutate(Location = factor(Location, levels=BC_LTHC_outbreaks_100Imputs[[100]]$Location[order(BHM)] ))
# order by increasing R0

# Just point estimates:
c<-ggplot(df2, aes(x=Location, y=Critical_time)) +
  geom_bar(aes(fill=Attack_rate/100), stat="identity") + ylab("Critical time (days)") +
  scale_fill_continuous(name = "Attack rate",  labels = percent)
c


# Including uncertainty
d <- ggplot(df2, aes(Location, Critical_time)) +
  geom_point(aes(colour = Attack_rate/100), size=2.5, stat="identity") +
  geom_errorbar(aes(ymin = Lower_CI, ymax = Upper_CI, colour = Attack_rate/100), width = 0.6, size=0.6) +
  scale_colour_continuous(name = "Attack rate",  labels = percent, type="gradient", low=palette()[2], high=palette()[1]) +
  ylab("Critical time (days)") + #coord_cartesian(ylim=c(0,100)) + 
  theme_sleek()
d



```

## Correlations between R0 estimates and additional LTHC facility covariate data

In this section we investigate any relationships between the facility R0 estimates and additional data obtained on the LTHC facilities (age of facility, room type etc.). The additional data is primarily contained in *italic*BC_OSABC_facilitydata.rda*italic*, and some factors we have loaded already (e.g. outbreak reported date).

First, we take a look at a few of these individually:

```{r}

# Take a look at the additional factor data:
print(BC_OSABC_facilitydata)


# Was the initial (by symptom onset) case staff?
# (2 outbreaks with multiple earliest symptom onsets, both resident & staff: 9 and 17.
# These are coded as unknown.)
staff_cat<-as.factor(BC_OSABC_facilitydata$`Identity of initial COVID-19 case`)

# Remove the 'unknown' categorisations to make this factor dichotomous, and calculate the correlation with R0
staff_cat_di <- staff_cat[staff_cat!="Unknown"]
staff_cat_di <- unclass(staff_cat_di)

print(paste0("BHM and identity of initial case correlation = ", round(cor(staff_cat_di, BHM[staff_cat!="Unknown"]),3)))
print(paste0("BHM multi-level and identity of initial case correlation = ", round(cor(staff_cat_di, BHM_multi[staff_cat!="Unknown"]),3)))
print(paste0("BHM and identity of initial case correlation = ", round(cor(staff_cat_di, EG[staff_cat!="Unknown"]),3)))
print(paste0("BHM and identity of initial case correlation = ", round(cor(unclass(staff_cat[staff_cat!="Unknown" & !is.na(ML)]), ML[staff_cat!="Unknown" & !is.na(ML)]),3))) #only non-NA ML ests


# Facility capacity
fac_cap <- BC_LTHC_outbreaks_100Imputs[[100]]$capacity
print(paste0("BHM and facility capacity correlation = ", round(cor(BHM, fac_cap),3)))
print(paste0("BHM multi-level and facility capacity correlation = ", round(cor(BHM_multi, fac_cap),3)))
print(paste0("EG and facility capacity correlation = ", round(cor(EG, fac_cap),3)))
print(paste0("ML and facility capacity correlation = ", round(cor(ML[!is.na(ML)], fac_cap[!is.na(ML)]),3)))


# outbreak reported date
rep_date <- BC_LTHC_outbreaks_100Imputs[[100]]$reported_date
rep_date <- as.numeric(as.POSIXct(rep_date, format="%Y-%m-%d %H:%M:%S", tz="GMT"))
print(paste0("BHM and facility capacity correlation = ", round(cor(BHM, rep_date),3)))
print(paste0("BHM multi-level and facility capacity correlation = ", round(cor(BHM_multi, rep_date),3)))
print(paste0("EG and facility capacity correlation = ", round(cor(EG, rep_date),3)))
print(paste0("ML and facility capacity correlation = ", round(cor(ML[!is.na(ML)], rep_date[!is.na(ML)]),3)))


```

Next, we create figures and tables for all factors available:

```{r, fig.width = 10, fig.height = 8}
 # Make a tibble with the loaded factor data, but also outbreak reported date and facility capacity.
cor_data <- as_tibble(BC_OSABC_facilitydata)
cor_data <- cbind(cor_data, fac_cap, rep_date)
names(cor_data)[(length(cor_data)-1):length(cor_data)] <- c("Facility capacity", "COVID-19 outbreak reported date")

# Create correlation table, for numeric covariates
rho_table <- cor_data %>%
  dplyr::select("Number of disease outbreaks 2018/19","Number of lodged complaints 2018/19",
         "Residents dependent for daily activities (%)","Average resident stay (days)",
         "Average resident age (years)","Direct care hours /resident/day",
         "Facility capacity", "COVID-19 outbreak reported date", "Year facility opened")
rho_table <-
  purrr::map2_df(rho_table,colnames(rho_table),function(x,var_name){
    cor.test(x,BHM) %>%
      broom::tidy() %>%
      mutate(Property = var_name)
  }) %>%
  dplyr::select(Property,everything())
# BHM correlations
rho_table %>%
  kableExtra::kable() %>%
  kableExtra::kable_styling(bootstrap_options = c("striped","responsive"))


# And correlations for dichotomous categorical factors (point-biserial)
# Initial case staff/resident (also already did this in last chunk)
# and
# Accreditation status
# Print these out:
print(paste0("BHM and identity of initial case (is staff) correlation = ", cor(unclass(as.factor(cor_data$"Identity of initial COVID-19 case"[cor_data$"Identity of initial COVID-19 case"!="Unknown"])), BHM[cor_data$"Identity of initial COVID-19 case"!="Unknown"])))
# Make this one negative since 'not accredited' is associated with a higher number: 
print(paste0("BHM and (positive) accreditation status correlation = ", -cor(unclass(as.factor(cor_data$"Accreditation status"[!is.na(cor_data$"Accreditation status")])), BHM[!is.na(cor_data$"Accreditation status")])))



## Plot correlations with R0

# separate numeric and character factors into 2 plots
cols_to_plot_num <- c(3, 6, 7, 8, 9, 10, 11, 13, 14)
cols_to_plot_cat <- c(2, 4, 5, 12)
cor_data <- cbind(cor_data, BHM)

cordatalong_num <- pivot_longer(cor_data, cols = all_of(cols_to_plot_num), names_to = "Factor", values_to = "Value")
cordatalong_cat <- pivot_longer(cor_data, cols = all_of(cols_to_plot_cat), names_to = "Factor", values_to = "Value")

p1 <- ggplot(data=cordatalong_num, aes(x=Value, y = BHM)) +
  geom_point(size=2, colour = "#E84646") + facet_wrap(~Factor,  scales = "free", ncol=4) + xlab("") + ylab(expression(R['0,k'])) +
  theme(text = element_text(size=12), axis.title.y = element_text(size = 15))

p2 <- ggplot(data=cordatalong_cat, aes(x=Value, y = BHM)) +
  geom_point(size=2, colour = "#E84646") + facet_wrap(~Factor,  scales = "free", ncol=4) + xlab("") + ylab(expression(R['0,k'])) +
  theme(text = element_text(size=12), axis.title.y = element_text(size = 15)) +
  theme(axis.text.x = element_text(angle = 45, hjust=1))
grid.arrange(grobs = list(p2, p1), nrow = 2, heights = c(1.1,3))
# (modified a few aesthetics of figure manually for paper - removed second R_0,k label, rearranged some titles that were cut off)



# If using paper colour scheme, reset the colour theme:
#ggthemr_reset()
```






